{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "A vector is a mathematical object that has both magnitude and direction. It is often represented as an arrow in a coordinate system, where the length of the arrow represents the magnitude and the direction of the arrow represents the direction.\n",
    "\n",
    "$$\n",
    "\\vec{v} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We will use [Numpy](https://numpy.org/) for fast vector operations.\n",
    "\n",
    "<img src=\"img/numpy.png\" alt=\"Numpy\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_functions import draw_vectors\n",
    "import numpy as np\n",
    "\n",
    "v = np.array([3, 4])\n",
    "\n",
    "draw_vectors([[3, 4, \"b\"]], [0, 5], [0, 5], \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Norm (Magnitude|Length)\n",
    "\n",
    "The norm of a vector is a measure of the length of the vector. It is calculated as the square root of the sum of the squares of the vector elements.\n",
    "\n",
    "$$\n",
    "\\| \\vec{v} \\| = \\sqrt{3^2 + 4^2}=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Normalization\n",
    "\n",
    "Normalization is the process of scaling individual samples to have unit norm. In case of vectors, it is the process of scaling a vector to have a length of 1. It is done by dividing the vector by its norm. The normalized vector is also called a unit vector written as $\\vec{v_0}$.\n",
    "\n",
    "$$\n",
    "\\vec{v_0} = \\frac{\\vec{v}}{\\| \\vec{v} \\|} = \\frac{\\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}}{5} = \\begin{pmatrix} 0.6 \\\\ 0.8 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\| \\vec{v_0} \\| = \\sqrt{0.6^2 + 0.8^2} = 1\n",
    "$$\n",
    "\n",
    "The direction of the vector keeps the same after normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = v / np.linalg.norm(v)\n",
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors([[3, 4, \"b\"], [0.6, 0.8, \"r\"]], [0, 5], [0, 5], \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Multiplication (Dot Product)\n",
    "\n",
    "The dot product of two vectors is a scalar quantity that is calculated as the sum of the products of the corresponding elements of the two vectors. It is also called the inner product. The dot product has a result of zero if the two vectors are orthogonal (perpendicular) to each other.\n",
    "\n",
    "$$\n",
    "\\vec{v} \\cdot \\vec{w} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} \\cdot \\begin{pmatrix} -4 \\\\ 3 \\end{pmatrix} = 3 \\cdot (-4) + 4 \\cdot 3 = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([-4, 3])\n",
    "a = np.array([2, 1])\n",
    "print(np.dot(v, w))  # static method\n",
    "print(v.dot(a))  # instance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_vectors([[3, 4, \"b\"], [-4, 3, \"r\"]], [-4, 3], [0, 6], \"orthogonal vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle Between Vectors\n",
    "\n",
    "The angle is independent of the magnitude of the vectors and is calculated as the inverse cosine of the dot product of the vectors divided by the product of the magnitudes of the vectors.\n",
    "\n",
    "![Angle Between Vectors](img/angle.png)\n",
    "\n",
    "$$\n",
    "\\cos(\\phi) = \\frac{\\vec{v} \\cdot \\vec{w}}{\\| \\vec{v} \\| \\cdot \\| \\vec{w} \\|} = \\text{cosine similarity}\n",
    "$$\n",
    "\n",
    "So if the magnitudes of the vectors are 1, the cosine similarity is the same as the dot product because the division by 1 does not change the value!\n",
    "\n",
    "$$\n",
    "\\cos(\\phi) = \\vec{v_0} \\cdot \\vec{w_0} = \\text{cosine similarity}\n",
    "$$\n",
    "\n",
    "#### Cosine Similarity\n",
    "\n",
    "The cosine similarity has a range of [-1,1].\n",
    "\n",
    "| Cosine Similarity | Angle |\n",
    "| --- | --- |\n",
    "| 1 | 0° |\n",
    "| 0 | 90° |\n",
    "| -1 | 180° |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Dimensions\n",
    "\n",
    "*Above you have seen nicely drawn 2D vectors. It is possible to draw 3D vectors as well, but it is not possible to draw 4D vectors and higher. But the math works the same way. The dot product and the angle between vectors are calculated the same way.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "An embedding model takes data like texts, images, or audio and converts it into a vector space. The vector space is a multi-dimensional space where each dimension represents a feature of the data. The embedding model learns to map the data into the vector space in such a way that similar data points are close to each other in the vector space.\n",
    "\n",
    "![Embedding](img/embedding.png)\n",
    "\n",
    "Source: https://jkfran.com/introduction-vector-embedding-databases.md\n",
    "\n",
    "Embeddings are used in many machine learning tasks, such as natural language processing, computer vision, and recommendation systems. Now it's easier to find similar items, compare items, and make predictions based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embeddings using [OpenAI's text-embedding-3-large model](https://openai.com/pricing)\n",
    "\n",
    "The text-embedding-3-large model is a transformer-based model that takes text as input and outputs a vector representation of the text. The model is trained on a large corpus of text data and learns to map the text into a vector space in such a way that similar texts are close to each other in the vector space.\n",
    "\n",
    "Each vector has **3072 dimensions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI  # OpenAI's Python client\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "\n",
    "# Define a helper function to calculate embeddings\n",
    "def get_embedding_vec(input):\n",
    "    \"\"\"Returns the embeddings vector for a given input\"\"\"\n",
    "    return (\n",
    "        client.embeddings.create(\n",
    "            input=input,\n",
    "            model=\"text-embedding-3-large\",  # We use the new embeddings model here (announced end of Jan 2024)\n",
    "            # dimensions=500  You can limit the number of output dimensions with the new embeddings models\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"dog\", \"cat\", \"animal\", \"house\", \"castle\", \"ship\", \"boat\"]\n",
    "embeddings = [get_embedding_vec(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information about an embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding of word \"dog\"\n",
    "\n",
    "print(\"Embedding:\", embeddings[0])\n",
    "print(\"Dimensions:\", len(embeddings[0]))\n",
    "print(\"Norm:\", np.linalg.norm(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the example below, the embedding is a list of 3072 float numbers. The model gives us vectors with a norm of nearly 1. The vectors are already normalized and the dot product can be used to calculate the cosine similarity.\n",
    "\n",
    "Now lets compare \"dog\" and \"boat\" with the other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = {\"word\": [], \"Cosine similarity\": []}\n",
    "\n",
    "for i in range(0, len(words)):\n",
    "    word = words[i]\n",
    "    embedding = embeddings[i]\n",
    "    similarity = np.dot(embeddings[0], embedding)  # Cosine similarity\n",
    "    comparisons[\"word\"].append(word)\n",
    "    comparisons[\"Cosine similarity\"].append(similarity)\n",
    "\n",
    "print(\"Dog\")\n",
    "pd.DataFrame(comparisons).sort_values(\"Cosine similarity\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = {\"word\": [], \"Cosine similarity\": []}\n",
    "\n",
    "for i in range(0, len(words)):\n",
    "    word = words[i]\n",
    "    embedding = embeddings[i]\n",
    "    similarity = np.dot(embeddings[len(words) - 1], embedding)  # Cosine similarity\n",
    "    comparisons[\"word\"].append(word)\n",
    "    comparisons[\"Cosine similarity\"].append(similarity)\n",
    "\n",
    "print(\"Boat\")\n",
    "pd.DataFrame(comparisons).sort_values(\"Cosine similarity\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Examples\n",
    "\n",
    "Here the strength of the embedding model becomes clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s):\n",
    "    sim1 = np.dot(get_embedding_vec(s[0]), get_embedding_vec(s[1]))\n",
    "    sim2 = np.dot(get_embedding_vec(s[0]), get_embedding_vec(s[2]))\n",
    "    return (sim1, sim2)\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    # Although sentences 1 and 2 use different words (soccer, football),\n",
    "    # the cosine similarity of 1 and 2 is higher compared to 1 and 3 because\n",
    "    # the meaning of 1 and 2 is more similar.\n",
    "    [\n",
    "        \"I enjoy playing soccer on weekends.\",\n",
    "        \"Football is my favorite sport. Playing it on weekends with friends helps me to relax.\",\n",
    "        \"In Austria, people often watch soccer on TV on weekends.\",\n",
    "    ],\n",
    "    # Here we test whether the OpenAI embedding model \"understands\", that the\n",
    "    # contextual meaning of \"Java\" is different in sentences 1 and 2. Therefore,\n",
    "    # the cosine similarity of 1 and 3 is higher as both are programming-related.\n",
    "    [\n",
    "        \"He is interested in Java programming.\",\n",
    "        \"He visited Java last summer.\",\n",
    "        \"He recently started learning Python programming.\",\n",
    "    ],\n",
    "    # The next example deals with negation handling. All three sentences are\n",
    "    # about whether someone likes going to the gym. Sentences 1 and 3 are positive\n",
    "    # (i.e. like training in the gym), while 2 is not. Therefore, 1 and 3 have\n",
    "    # a higher cosine similarity.\n",
    "    [\n",
    "        \"I like going to the gym.\",\n",
    "        \"I don't like going to the gym.\",\n",
    "        \"I don't dislike going to the gym.\",\n",
    "    ],\n",
    "    # Let's take a look at ideomatic expressions. Sentences 1 and 2 have very\n",
    "    # similar meaning. 3 also contains \"cats and dogs\", but the meaning is different.\n",
    "    # As a result, cosine similarity between 1 and 2 is higher.\n",
    "    [\n",
    "        \"It's raining cats and dogs.\",\n",
    "        \"The weather is very bad, it's pouring outside.\",\n",
    "        \"Cats and dogs don't go outside when it rains.\",\n",
    "    ],\n",
    "    # The last example demonstrates the limits of embeddings. Berry Harris is\n",
    "    # a well-known teacher in Jazz. Using \"the 6th on the 5th\" is typical for him.\n",
    "    # One must know Berry Harris and the musical theory that he has tought to\n",
    "    # understand the similarity of the sentences 1 and 2. OpenAI embeddings\n",
    "    # do not understand that.\n",
    "    [\n",
    "        \"I like how Barry Harris described Jazz theory.\",\n",
    "        \"Playing the 6th on the 5th is an important concept that you must understand.\",\n",
    "        \"My friends Barry and Harris often visit me to play computer games.\",\n",
    "    ],\n",
    "]\n",
    "print(f\"Semantic similarity: {compare(sentences[0])}\")\n",
    "print(f\"Contextual meaning: {compare(sentences[1])}\")\n",
    "print(f\"Negation handling: {compare(sentences[2])}\")\n",
    "print(f\"Idiomatic expressions: {compare(sentences[3])}\")\n",
    "print(f\"Knowledge: {compare(sentences[4])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My trip advisor\n",
    "\n",
    "I will use the text-embedding-3-large model to create a trip advisor. The user can input a text and the model will find the most similar destinations in the database. The database contains 9 different locations with a description. The model will return the most similar locations to the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [\n",
    "    \"Maldives, Maldives: A tropical paradise with pristine beaches. Activities: Snorkeling, Scuba diving, Relaxing on the beach.\",\n",
    "    \"Paris, France: The City of Love, famous for its iconic landmarks and art. Activities: Eiffel Tower visit, Louvre Museum, Seine River cruise.\",\n",
    "    \"Maui, Hawaii: A beautiful island with stunning beaches and lush landscapes. Activities: Surfing, Hiking, Whale watching.\",\n",
    "    \"Cancun, Mexico: A popular beach destination with crystal-clear waters. Activities: Swimming, Snorkeling, Exploring Mayan ruins.\",\n",
    "    \"Tokyo, Japan: A vibrant metropolis with a mix of modern and traditional attractions. Activities: Shibuya crossing, Tokyo Disneyland, Tsukiji Fish Market.\",\n",
    "    \"Banff National Park, Canada: A breathtaking national park with stunning mountain scenery. Activities: Hiking, Wildlife spotting, Canoeing.\",\n",
    "    \"Norwegian Fjords, Norway: A picturesque region with majestic fjords and waterfalls. Activities: Fjord cruises, Hiking, Kayaking.\",\n",
    "    \"New York City, USA: The Big Apple, known for its skyscrapers, museums, and diverse culture. Activities: Times Square, Central Park, Statue of Liberty visit.\",\n",
    "    \"Plitvice Lakes National Park, Croatia: A natural wonderland with cascading lakes and waterfalls. Activities: Walking trails, Boat rides, Wildlife watching.\",\n",
    "]\n",
    "\n",
    "destinations = []\n",
    "\n",
    "for description in descriptions:\n",
    "    parts = description.split(\": \")\n",
    "    destinations.append(\n",
    "        {\n",
    "            \"Name\": parts[0],\n",
    "            \"Description\": parts[1],\n",
    "            \"Embedding\": get_embedding_vec(parts[1]),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_values = [\n",
    "    \"My next holidays should be in a sunny place with a nice beach\",\n",
    "    \"I want to see beautiful nature\",\n",
    "    \"In the next summer I would like to visit a famous city with a lot of culture\",\n",
    "]\n",
    "\n",
    "number = 0  # 0, 1, 2\n",
    "\n",
    "search_embedding = get_embedding_vec(search_values[number])\n",
    "\n",
    "data = {\"Name\": [], \"Similarity\": []}\n",
    "for destination in destinations:\n",
    "    similarity = np.dot(search_embedding, destination[\"Embedding\"])\n",
    "    data[\"Name\"].append(destination[\"Name\"])\n",
    "    data[\"Similarity\"].append(similarity)\n",
    "print(f\"Search: {search_values[number]}\")\n",
    "pd.DataFrame(data).sort_values(\"Similarity\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search and the RAG Pattern\n",
    "\n",
    "Embeddings play a crucial role in _Retrieval-Augmented Generation_ (_RAG_) solutions, an approach in artificial intelligence that combines the capabilities of information retrieval and text generation. In RAG systems, embeddings are used to retrieve relevant information from large datasets or knowledge bases. It is not necessary for these databases to have been included in the original training of the embeddings models. They can be internal data sets that are not publicly accessible on the internet.\n",
    "\n",
    "In RAG solutions, queries or input texts are transformed into embeddings. Then, the cosine similarity to the document embeddings existing in the database is calculated to identify the most relevant text sections from the database. These retrieved pieces of information are then used by a text generation model like _ChatGPT_ to generate contextually relevant answers or content.\n",
    "\n",
    "_Vector databases_ play a central role in the functioning of RAG systems. They are designed to efficiently store, index, and query high-dimensional vectors. In the context of RAG solutions and similar systems, vector databases serve as a storage for the embeddings of documents or data pieces that come from a large pool of information. When a user makes a query, this query is first transformed into an embedding vector. The vector database is then used to quickly find the vectors that correspond most closely to this query vector - that is, those documents or pieces of information that show the highest similarity. This process of quickly finding similar vectors in large datasets is known as _Nearest Neighbor Search_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_values = [\n",
    "    \"My next holidays should be in a sunny place with a nice beach\",\n",
    "    \"I want to see beautiful nature\",\n",
    "    \"In the next summer I would like to visit a famous city with a lot of culture\",\n",
    "    \"What is the name of Bart Simpsons sister?\",\n",
    "]\n",
    "\n",
    "number = 3  # 0, 1, 2\n",
    "\n",
    "search_embedding = get_embedding_vec(search_values[number])\n",
    "\n",
    "data = {\"Destination\": [], \"Similarity\": []}\n",
    "for destination in destinations:\n",
    "    similarity = np.dot(search_embedding, destination[\"Embedding\"])\n",
    "    data[\"Destination\"].append(destination[\"Name\"] + \": \" + destination[\"Description\"])\n",
    "    data[\"Similarity\"].append(similarity)\n",
    "print(f\"Search: {search_values[number]}\")\n",
    "sorted_result = list(\n",
    "    pd.DataFrame(data).sort_values(\"Similarity\", ascending=False).head(3)[\"Destination\"]\n",
    ")\n",
    "sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "t = Template(\n",
    "    \"\"\"\n",
    "You are a helpful assistant in a travel agency. Customers are describing\n",
    "what they want to do in their vacation. Make suggestions based on the\n",
    "city descriptions provided below. ONLY use the provided city descriptions.\n",
    "Do NOT use other information sources.\n",
    "\n",
    "If you cannot generate a meaningful answer based on the given city description,\n",
    "say \"Sorry, I cannot help\". If the user's input is not related to finding\n",
    "a travel location, say \"Sorry, I can only help with vacation locations\".\n",
    "\n",
    "===========\n",
    "$options\n",
    "===========\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "system_prompt = t.substitute(options=\"\\n\\n\".join([item for item in sorted_result[:3]]))\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ChatGPT-4 Answer:\")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": search_values[number],\n",
    "        },\n",
    "    ],\n",
    "    model=\"gpt-4-1106-preview\",\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Open Source Embedding models\n",
    "\n",
    "- [sentence-transformers](https://www.sbert.net)\n",
    "- [transformers](https://huggingface.co/transformers/)\n",
    "\n",
    "It is possible to use the sentence-transformers library to create embeddings for text data and images. The library provides pre-trained models that can be used to create embeddings for text data and images and is based on PyTorch. The library also provides a simple API for creating embeddings and performing similarity searches. The models give you the vector as a numpy array so the cosine similarity can be calculated easily.\n",
    "\n",
    "Just install the python library with pip:\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "dog = model.encode(\"dog\")\n",
    "\n",
    "print(\"Embedding:\", dog)\n",
    "print(\"Dimensions:\", len(dog))\n",
    "print(\"Norm:\", np.linalg.norm(dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = model.encode(\"cat\")\n",
    "\n",
    "words = [\"dog\", \"cat\", \"animal\", \"house\", \"castle\", \"ship\", \"boat\"]\n",
    "print(dog.dot(cat))  # Cosine similarity with numpy dot function\n",
    "\n",
    "# use sentence-transformers to calculate cosine similarity\n",
    "\n",
    "embeddings = model.encode(words)\n",
    "\n",
    "util.dot_score(dog, embeddings).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our examples from above with the sentence-transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s):\n",
    "    sim1 = np.dot(model.encode(s[0]), model.encode(s[1]))\n",
    "    sim2 = np.dot(model.encode(s[0]), model.encode(s[2]))\n",
    "    return (sim1, sim2)\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    [\n",
    "        \"I enjoy playing soccer on weekends.\",\n",
    "        \"Football is my favorite sport. Playing it on weekends with friends helps me to relax.\",\n",
    "        \"In Austria, people often watch soccer on TV on weekends.\",\n",
    "    ],\n",
    "    [\n",
    "        \"He is interested in Java programming.\",\n",
    "        \"He visited Java last summer.\",\n",
    "        \"He recently started learning Python programming.\",\n",
    "    ],\n",
    "    [\n",
    "        \"I like going to the gym.\",\n",
    "        \"I don't like going to the gym.\",\n",
    "        \"I don't dislike going to the gym.\",\n",
    "    ],\n",
    "    [\n",
    "        \"It's raining cats and dogs.\",\n",
    "        \"The weather is very bad, it's pouring outside.\",\n",
    "        \"Cats and dogs don't go outside when it rains.\",\n",
    "    ],\n",
    "    [\n",
    "        \"I like how Barry Harris described Jazz theory.\",\n",
    "        \"Playing the 6th on the 5th is an important concept that you must understand.\",\n",
    "        \"My friends Barry and Harris often visit me to play computer games.\",\n",
    "    ],\n",
    "]\n",
    "print(f\"Semantic similarity: {compare(sentences[0])}\")\n",
    "print(f\"Contextual meaning: {compare(sentences[1])}\")\n",
    "print(f\"Negation handling: {compare(sentences[2])}\")\n",
    "print(f\"Idiomatic expressions: {compare(sentences[3])}\")\n",
    "print(f\"Knowledge: {compare(sentences[4])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Databases\n",
    "\n",
    "![Vector Database](img/qdrant.png)\n",
    "\n",
    "Vector databases are databases that store vectors and allow you to perform similarity searches on the vectors. They are used to store and search for embeddings. The Qdrant vector database is an open-source vector database written in Rust.\n",
    "\n",
    "Vector database improve the performance of similarity searches by using algorithms that are optimized for searching in high-dimensional vector spaces.\n",
    "\n",
    "Here you can see how to use the Qdrant vector database to store and search for embeddings.\n",
    "\n",
    "**Start docker container:**\n",
    "\n",
    "```bash\n",
    "docker container run --name qdrant -p 6333:6333 qdrant/qdrant\n",
    "```\n",
    "\n",
    "### Connect to the database and create collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient  # Qdrant's Python client\n",
    "\n",
    "qdrant = QdrantClient(\"localhost\", port=6333)  # Connect to Qdrant\n",
    "\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "qdrant.create_collection(\n",
    "    collection_name=\"destinations\",\n",
    "    vectors_config=VectorParams(\n",
    "        size=3072, distance=Distance.DOT\n",
    "    ),  # Could be Distance.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "points = []\n",
    "\n",
    "# add points to my array\n",
    "\n",
    "for i, destination in enumerate(destinations):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=destination[\"Embedding\"],\n",
    "            payload={\"destination\": destination[\"Name\"]},\n",
    "        )\n",
    "    )\n",
    "\n",
    "# add points to the collection\n",
    "\n",
    "operation_info = qdrant.upsert(\n",
    "    collection_name=\"destinations\",\n",
    "    wait=True,\n",
    "    points=points,\n",
    ")\n",
    "\n",
    "print(operation_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = qdrant.search(\n",
    "    collection_name=\"destinations\",\n",
    "    query_vector=get_embedding_vec(\"I want to see beautiful nature\"),\n",
    "    limit=3,\n",
    ")\n",
    "\n",
    "search_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
